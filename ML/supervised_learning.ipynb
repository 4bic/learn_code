{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fruit_label</th>\n",
       "      <th>fruit_name</th>\n",
       "      <th>fruit_subtype</th>\n",
       "      <th>mass</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>color_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>192</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>180</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>granny_smith</td>\n",
       "      <td>176</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>86</td>\n",
       "      <td>6.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>mandarin</td>\n",
       "      <td>84</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fruit_label fruit_name fruit_subtype  mass  width  height  color_score\n",
       "0            1      apple  granny_smith   192    8.4     7.3         0.55\n",
       "1            1      apple  granny_smith   180    8.0     6.8         0.59\n",
       "2            1      apple  granny_smith   176    7.4     7.2         0.60\n",
       "3            2   mandarin      mandarin    86    6.2     4.7         0.80\n",
       "4            2   mandarin      mandarin    84    6.0     4.6         0.79"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "fruits = pd.read_table('fruit_data_with_colors.txt')\n",
    "fruits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple binary clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[3. 4. 2. 1. 3. 4. 3. 3. 4. 1. 4. 3. 1. 2. 3. 1. 4. 1. 4. 1. 1. 3. 1. 4.\n 4. 4. 3. 1. 1. 4. 3. 2. 1. 3. 1. 1. 1. 3. 4. 2. 1. 4. 4. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c9ac2e68ab14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# apply the scaling to the test set that's computed for the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mX_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scale_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[3. 4. 2. 1. 3. 4. 3. 3. 4. 1. 4. 3. 1. 2. 3. 1. 4. 1. 4. 1. 1. 3. 1. 4.\n 4. 4. 3. 1. 1. 4. 3. 2. 1. 3. 1. 1. 1. 3. 4. 2. 1. 4. 4. 4.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "feature_names_fruits = ['height', 'width', 'mass', 'color_score']\n",
    "X_fruits = fruits[feature_names_fruits]\n",
    "y_fruits = fruits['fruit_label']\n",
    "target_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n",
    "\n",
    "X_fruits_2d = fruits[['height', 'width']]\n",
    "y_fruits_2d = fruits['fruit_label']\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X_fruits, y_fruits, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# apply the scaling to the test set that's computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train_scaled, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test_scaled, y_test)))\n",
    "\n",
    "example_fruit = [[5.5, 2.2, 10, 0.70]]\n",
    "print('Predicted fruit type for ', example_fruit, ' is ', \n",
    "      target_names_fruits[knn.predict(example_fruit)[0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## working with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from adspy_shared_utilities import load_crime_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n",
    "\n",
    "# synthetic dataset for simple regression\n",
    "from sklearn.datasets import make_regression\n",
    "plt.figure()\n",
    "plt.title('Sample regresion problem with one input variable')\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "plt.scatter(X_R1, y_R1, marker='o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## More complex regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Complex regression problem with one input variable')\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "plt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Binary classification with two informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_classification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4535804d764e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample binary classification with two informative features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m X_C2, y_C2 = make_classification(n_samples = 100, n_features = 2,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                 \u001b[0mn_redundant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_informative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mn_clusters_per_class\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflip_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_classification' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b0286d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('sample binary classification with two informative features')\n",
    "\n",
    "X_C2, y_C2 = make_classification(n_samples = 100, n_features = 2,\n",
    "                                n_redundant = 0, n_informative = 2,\n",
    "                                n_clusters_per_class =1, flip_y = 0.1,\n",
    "                                class_sep = 0.5, random_state= 0 )\n",
    "\n",
    "plt.scatter(X_C2[:, 0], X_C2[:, 1],c=y_C2,marker='o',s=50,cmap=cmap_bold)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Binary classification with classes that are not linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('sample binary classification problem with non-linearly separable classes')\n",
    "X_D2, y_D2 = make_blobs(n_samples = 100, n_features=2, centers=8,\n",
    "                       cluster_std=1.3, random_state=4)\n",
    "y_D2 = y_D2 % 2\n",
    "\n",
    "plt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2, marker='o', s=50, cmap=cmap_bold)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Breast cancer dataset for classification\n",
    "cancer = load_breast_cancer()\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Communities and Crime dataset\n",
    "crime = load_crime_dataset()\n",
    "(X_crime, y_crime) = load_crime_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_C2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8406ed8e047f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0madspy_shared_utilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_two_class_knn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                                    random_state=0)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_C2' is not defined"
     ]
    }
   ],
   "source": [
    "from adspy_shared_utilities import plot_two_class_knn\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
    "                                                   random_state=0)\n",
    "\n",
    "plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)\n",
    "plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state=0)\n",
    "\n",
    "knnreg = KNeighborsRegressor(n_neighbors = 5).fit(X_train, y_train)\n",
    "\n",
    "print (knnreg.predict(X_test))\n",
    "print('R-squared test score: {:.3f}'.format(knnreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, subaxes = plt.subplots(1,2, figsize=(8,4))\n",
    "X_predict_input = np.linspace(-3,3,50).reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1[0::5], y_R1[0::5], random_state=0)\n",
    "\n",
    "for thisaxis, K in zip(subaxes, [1,3]):\n",
    "    knnreg=KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    thisaxis.set_xlim([-2.5, 0.75])\n",
    "    thisaxis.plot(X_predict_input,y_predict_output, '^', markersize=10,\n",
    "                 label='Predicted', alpha=0.8)\n",
    "    thisaxis.plot(X_train, y_train, 'o',label='True Value', alpha=0.8)\n",
    "    thisaxis.set_xlabel('Input feature')\n",
    "    thisaxis.set_ylabel('Target Value')\n",
    "    thisaxis.set_title('K-NN regression (K={})'.format(K))\n",
    "    thisaxis.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model complexity as a function of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, subaxes = plt.subplots(5, 1, figsize=(5,20))\n",
    "X_predict_input = np.linspace(-3, 3, 500).reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "for thisaxis, K in zip(subaxes, [1, 3, 7, 15, 55]):\n",
    "    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n",
    "    y_predict_output = knnreg.predict(X_predict_input)\n",
    "    train_score = knnreg.score(X_train, y_train)\n",
    "    test_score = knnreg.score(X_test, y_test)\n",
    "    thisaxis.plot(X_predict_input, y_predict_output)\n",
    "    thisaxis.plot(X_train, y_train, 'o', alpha=0.9, label='Train')\n",
    "    thisaxis.plot(X_test, y_test, '^', alpha=0.9, label='Test')\n",
    "    thisaxis.set_xlabel('Input feature')\n",
    "    thisaxis.set_ylabel('Target value')\n",
    "    thisaxis.set_title('KNN Regression (K={})\\n\\\n",
    "Train $R^2 = {:.3f}$,  Test $R^2 = {:.3f}$'\n",
    "                      .format(K, train_score, test_score))\n",
    "    thisaxis.legend()\n",
    "    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear models for regression\n",
    "\n",
    "### linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression: example plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker='o', s=50, alpha=0.8)\n",
    "plt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\n",
    "plt.title('Least Squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target Value(y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression with feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "clf = Ridge().fit(X_train_scaled, y_train)\n",
    "r2_score = clf.score(X_test_scaled, y_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "    r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "          .format(this_alpha, num_coeff_bigger, r2_train, r2_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X_crime), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression with regularization parameter: alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\n\\\n",
    "polynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_F1_poly = poly.fit_transform(X_F1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\n\\\n",
    "overfitting, so we often use polynomial features in combination\\n\\\n",
    "with regression that has a regularization penalty, like ridge\\n\\\n",
    "regression.\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models for classification\n",
    "\n",
    "\n",
    "### Logistic regression\n",
    "\n",
    "\n",
    "#### Logistic regression for binary classification on fruits dataset using height, width features (positive class: apple, negative class: others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import (\n",
    "plot_class_regions_for_classifier_subplot)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "y_fruits_apple = y_fruits_2d == 1   # make into a binary problem: apples vs everything else\n",
    "X_train, X_test, y_train, y_test = (\n",
    "train_test_split(X_fruits_2d.as_matrix(),\n",
    "                y_fruits_apple.as_matrix(),\n",
    "                random_state = 0))\n",
    "\n",
    "clf = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None,\n",
    "                                         None, 'Logistic regression \\\n",
    "for binary classification\\nFruit dataset: Apple vs others',\n",
    "                                         subaxes)\n",
    "\n",
    "h = 6\n",
    "w = 8\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    "     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
    "\n",
    "h = 10\n",
    "w = 7\n",
    "print('A fruit with height {} and width {} is predicted to be: {}'\n",
    "     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n",
    "subaxes.set_xlabel('height')\n",
    "subaxes.set_ylabel('width')\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic regression on simple synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from adspy_shared_utilities import (\n",
    "plot_class_regions_for_classifier_subplot)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "title = 'Logistic regression, simple synthetic dataset C = {:.3f}'.format(1.0)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                         None, None, title, subaxes)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Logistic regression regularization: C parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (\n",
    "train_test_split(X_fruits_2d.as_matrix(),\n",
    "                y_fruits_apple.as_matrix(),\n",
    "                random_state=0))\n",
    "\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(4, 10))\n",
    "\n",
    "for this_C, subplot in zip([0.1, 1, 100], subaxes):\n",
    "    clf = LogisticRegression(C=this_C).fit(X_train, y_train)\n",
    "    title ='Logistic regression (apple vs rest), C = {:.3f}'.format(this_C)\n",
    "    \n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             X_test, y_test, title,\n",
    "                                             subplot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Application to real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "print('Breast cancer dataset')\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "\n",
    "\n",
    "### Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
    "\n",
    "fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "this_C = 1.0\n",
    "clf = SVC(kernel = 'linear', C=this_C).fit(X_train, y_train)\n",
    "title = 'Linear SVC, C = {:.3f}'.format(this_C)\n",
    "plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear Support Vector Machine: C parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for this_C, subplot in zip([0.00001, 100], subaxes):\n",
    "    clf = LinearSVC(C=this_C).fit(X_train, y_train)\n",
    "    title = 'Linear SVC, C = {:.5f}'.format(this_C)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             None, None, title, subplot)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Application LSVC to real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = LinearSVC().fit(X_train, y_train)\n",
    "print('Breast cancer dataset')\n",
    "print('Accuracy of Linear SVC classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Linear SVC classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multi-class classification with linear models\n",
    "\n",
    "\n",
    "### LinearSVC with M classes generates M one vs rest classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state = 0)\n",
    "\n",
    "clf = LinearSVC(C=5, random_state = 67).fit(X_train, y_train)\n",
    "print('Coefficients:\\n', clf.coef_)\n",
    "print('Intercepts:\\n', clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multi-class results on the fruit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "colors = ['r', 'g', 'b', 'y']\n",
    "cmap_fruits = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#FFFF00'])\n",
    "\n",
    "plt.scatter(X_fruits_2d[['height']], X_fruits_2d[['width']],\n",
    "           c=y_fruits_2d, cmap=cmap_fruits, edgecolor = 'black', alpha=.7)\n",
    "\n",
    "x_0_range = np.linspace(-10, 15)\n",
    "\n",
    "for w, b, color in zip(clf.coef_, clf.intercept_, ['r', 'g', 'b', 'y']):\n",
    "    # Since class prediction with a linear model uses the formula y = w_0 x_0 + w_1 x_1 + b, \n",
    "    # and the decision boundary is defined as being all points with y = 0, to plot x_1 as a \n",
    "    # function of x_0 we just solve w_0 x_0 + w_1 x_1 + b = 0 for x_1:\n",
    "    plt.plot(x_0_range, -(x_0_range * w[0] + b) / w[1], c=color, alpha=.8)\n",
    "    \n",
    "plt.legend(target_names_fruits)\n",
    "plt.xlabel('height')\n",
    "plt.ylabel('width')\n",
    "plt.xlim(-2, 12)\n",
    "plt.ylim(-2, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kernelized Support Vector Machines\n",
    "\n",
    "\n",
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "\n",
    "# The default SVC kernel is radial basis function (RBF)\n",
    "plot_class_regions_for_classifier(SVC().fit(X_train, y_train),\n",
    "                                 X_train, y_train, None, None,\n",
    "                                 'Support Vector Classifier: RBF kernel')\n",
    "\n",
    "# Compare decision boundries with polynomial kernel, degree = 3\n",
    "plot_class_regions_for_classifier(SVC(kernel = 'poly', degree = 3)\n",
    "                                 .fit(X_train, y_train), X_train,\n",
    "                                 y_train, None, None,\n",
    "                                 'Support Vector Classifier: Polynomial kernel, degree = 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Support Vector Machine with RBF kernel: gamma parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_class_regions_for_classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(3, 1, figsize=(4, 11))\n",
    "\n",
    "for this_gamma, subplot in zip([0.01, 1.0, 10.0], subaxes):\n",
    "    clf = SVC(kernel = 'rbf', gamma=this_gamma).fit(X_train, y_train)\n",
    "    title = 'Support Vector Classifier: \\nRBF kernel, gamma = {:.2f}'.format(this_gamma)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                             None, None, title, subplot)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Support Vector Machine with RBF kernel: using both C and gamma parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "fig, subaxes = plt.subplots(3, 4, figsize=(15, 10), dpi=50)\n",
    "\n",
    "for this_gamma, this_axis in zip([0.01, 1, 5], subaxes):\n",
    "    \n",
    "    for this_C, subplot in zip([0.1, 1, 15, 250], this_axis):\n",
    "        title = 'gamma = {:.2f}, C = {:.2f}'.format(this_gamma, this_C)\n",
    "        clf = SVC(kernel = 'rbf', gamma = this_gamma,\n",
    "                 C = this_C).fit(X_train, y_train)\n",
    "        plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n",
    "                                                 X_test, y_test, title,\n",
    "                                                 subplot)\n",
    "        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Application of SVMs to a real dataset: unnormalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "clf = SVC(C=10).fit(X_train, y_train)\n",
    "print('Breast cancer dataset (unnormalized features)')\n",
    "print('Accuracy of RBF-kernel SVC on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of RBF-kernel SVC on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Application of SVMs to a real dataset: \n",
    "\n",
    "### normalized data with feature preprocessing using minmax scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = SVC(C=10).fit(X_train_scaled, y_train)\n",
    "print('Breast cancer dataset (normalized with MinMax scaling)')\n",
    "print('RBF-kernel SVC (with MinMax scaling) training set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('RBF-kernel SVC (with MinMax scaling) test set accuracy: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "\n",
    "### Example based on k-NN classifier with fruit dataset (2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors = 5)\n",
    "X = X_fruits_2d.as_matrix()\n",
    "y = y_fruits_2d.as_matrix()\n",
    "cv_scores = cross_val_score(clf, X, y)\n",
    "\n",
    "print('Cross-validation scores (3-fold):', cv_scores)\n",
    "print('Mean cross-validation score (3-fold): {:.3f}'\n",
    "     .format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on performing cross-validation for more advanced scenarios.\n",
    "\n",
    "In some cases (e.g. when feature values have very different ranges), we've seen the need to scale or normalize the training and test sets before use with a classifier. The proper way to do cross-validation when you need to scale the data is not to scale the entire dataset with a single transform, since this will indirectly leak information into the training data about the whole dataset, including the test data (see the lecture on data leakage later in the course). Instead, scaling/normalizing must be computed and applied for each cross-validation fold separately. To do this, the easiest way in scikit-learn is to use pipelines. While these are beyond the scope of this course, further information is available in the scikit-learn documentation here:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "or the Pipeline section in the recommended textbook: Introduction to Machine Learning with Python by Andreas C. Mller and Sarah Guido (O'Reilly Media)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Validation curve example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "param_range = np.logspace(-3, 3, 4)\n",
    "train_scores, test_scores = validation_curve(SVC(), X, y,\n",
    "                                            param_name='gamma',\n",
    "                                            param_range=param_range, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code based on scikit-learn validation_plot example\n",
    "#  See:  http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html\n",
    "plt.figure()\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title('Validation Curve with SVM')\n",
    "plt.xlabel('$\\gamma$ (gamma)')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "\n",
    "plt.semilogx(param_range, train_scores_mean, label='Training score',\n",
    "            color='darkorange', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                color='darkorange', lw=lw)\n",
    "\n",
    "plt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n",
    "            color='navy', lw=lw)\n",
    "\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                color='navy', lw=lw)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 3)\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setting max decision tree depth to help avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf2.score(X_train, y_train)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Vizualizing decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_tree(clf, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pre-pruned version (max_depth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_tree(clf2, iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import plot_feature_importances\n",
    "\n",
    "plt.figure(figsize=(10,4), dpi=80)\n",
    "plot_feature_importances(clf, iris.feature_names)\n",
    "plt.show()\n",
    "\n",
    "print('Feature importances: {}'.format(clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_class_regions_for_classifier_subplot\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 0)\n",
    "fig, subaxes = plt.subplots(6, 1, figsize=(6, 32))\n",
    "\n",
    "pair_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\n",
    "tree_max_depth = 4\n",
    "\n",
    "for pair, axis in zip(pair_list, subaxes):\n",
    "    X = X_train[:, pair]\n",
    "    y = y_train\n",
    "    \n",
    "    clf = DecisionTreeClassifier(max_depth=tree_max_depth).fit(X, y)\n",
    "    title = 'Decision Tree, max_depth = {:d}'.format(tree_max_depth)\n",
    "    plot_class_regions_for_classifier_subplot(clf, X, y, None,\n",
    "                                             None, title, axis,\n",
    "                                             iris.target_names)\n",
    "    \n",
    "    axis.set_xlabel(iris.feature_names[pair[0]])\n",
    "    axis.set_ylabel(iris.feature_names[pair[1]])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision Trees on a real-world dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from adspy_shared_utilities import plot_decision_tree\n",
    "from adspy_shared_utilities import plot_feature_importances\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 8,\n",
    "                            random_state = 0).fit(X_train, y_train)\n",
    "\n",
    "plot_decision_tree(clf, cancer.feature_names, cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Breast cancer dataset: decision tree')\n",
    "print('Accuracy of DT classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of DT classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "plt.figure(figsize=(10,6),dpi=80)\n",
    "plot_feature_importances(clf, cancer.feature_names)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
